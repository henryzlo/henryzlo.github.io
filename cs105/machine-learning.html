<h1>Machine Learning</h1>

Machine learning deals with learning models from data to solve problems.

<h2>Basic Problems</h2>
<ul>
  <li>The three basic problems studied in machine learning are classification, regression, and clustering.</li>
  <li>Classification is the problem of determining whether something belongs to one group, or another.
    <ul>
      <li>Face recognition is the problem of classifying an image as a face, or not.</li>
      <li>Determining whether a car might break down or not is the problem of classifying the car as dangerous, or not, given some information about the vehicle.</li>
      <li>Deciding what ads to show you is the problem of classifying a product to be potentially interesting or not, given some information about you and the product.</li>
      <li>We can have multiple classes: whether an image is a bird, plane, turtle, etc.</li>
    </ul>
  </li>
  <li>Regression is the problem of predicting some value, based on some information.
    <ul>
      <li>Predicting what a stock price is in the future is a regression problem.</li>
      <li>Estimating how much calories you are burning from exercise is a regression problem.</li>
      <li>Estimating how much money you will spend shopping on Black Friday is a regression problem.</li>
    </ul>
  </li>
  <li>Clustering is the problem of grouping data together.
    <ul>
      <li>For example, finding areas around the city where people tend to congregate on Fridays.</li>
      <li>Finding groups of friends on Facebook.</li>
      <li>Finding plant species which are likely to be closely related.</li>
    </ul>
  </li>
</ul>

<h2>Problem Formulation</h2>
<ul>
  <li>We have general models, and learn by feeding input data to them.</li>
  <li>Data is in a form called a "feature vector".
    <ul>
      <li>This is essentially a list of numbers (features).</li>
      <li>For example, if we want to determine someone's gender, then some features might be height, weight.</li>
      <li>We can have any number of features, but some features are irrelevant (e.g. eye color), and don't help.</li>
      <li>Sometimes we have to turn inputs (e.g. images, sounds) into feature vectors, because they are not lists of numbers.</li>
    </ul>
  </li>
</ul>

<h2>Iris Example</h2>
<ul>
  <li>
    As an example, we consider the iris data set.
    <ul>
      <li>Each row in this data set is a feature vector.</li>
      <li>The features are the four measurements (sepal length and width, petal length and width)
	<table cellpadding="5" cellspacing="0" border="1px" style="border: 1px solid #999966; background: #ffffff; margin-top:0.5em;" class="wikitable sortable jquery-tablesorter">
<caption>Fisher's <i>Iris</i> Data</caption>
<thead><tr bgcolor="#E7DCC3">
<th class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending">Sepal length</th>
<th class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending">Sepal width</th>
<th class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending">Petal length</th>
<th class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending">Petal width</th>
<th class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending">Species</th>
</tr></thead><tbody>
<tr>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>5.4</td>
<td>3.9</td>
<td>1.7</td>
<td>0.4</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>4.3</td>
<td>3.0</td>
<td>1.1</td>
<td>0.1</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>5.8</td>
<td>4.0</td>
<td>1.2</td>
<td>0.2</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>5.7</td>
<td>4.4</td>
<td>1.5</td>
<td>0.4</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>5.4</td>
<td>3.9</td>
<td>1.3</td>
<td>0.4</td>
<td><i>I.&nbsp;setosa</i></td>
</tr>
<tr>
<td>5.7</td>
<td>2.9</td>
<td>4.2</td>
<td>1.3</td>
<td><i>I.&nbsp;versicolor</i></td>
</tr>
<tr>
<td>6.2</td>
<td>2.9</td>
<td>4.3</td>
<td>1.3</td>
<td><i>I.&nbsp;versicolor</i></td>
</tr>
<tr>
<td>5.1</td>
<td>2.5</td>
<td>3.0</td>
<td>1.1</td>
<td><i>I.&nbsp;versicolor</i></td>
</tr>
<tr>
<td>5.7</td>
<td>2.8</td>
<td>4.1</td>
<td>1.3</td>
<td><i>I.&nbsp;versicolor</i></td>
</tr>
<tr>
<td>6.3</td>
<td>3.3</td>
<td>6.0</td>
<td>2.5</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>6.5</td>
<td>3.0</td>
<td>5.8</td>
<td>2.2</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>7.6</td>
<td>3.0</td>
<td>6.6</td>
<td>2.1</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>4.9</td>
<td>2.5</td>
<td>4.5</td>
<td>1.7</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>7.3</td>
<td>2.9</td>
<td>6.3</td>
<td>1.8</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>6.7</td>
<td>2.5</td>
<td>5.8</td>
<td>1.8</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>7.2</td>
<td>3.6</td>
<td>6.1</td>
<td>2.5</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>6.5</td>
<td>3.2</td>
<td>5.1</td>
<td>2.0</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
<tr>
<td>6.4</td>
<td>2.7</td>
<td>5.3</td>
<td>1.9</td>
<td><i>I.&nbsp;virginica</i></td>
</tr>
</tbody><tfoot></tfoot></table><br/>
      </li>
    </ul>
  </li>
  <li>Clustering example: We know that there are 3 species.  Clustering can find the "blobs" of data that might correspond to the species.
    <ul>
      <li>Clustering algorithm can find the three clusters of data.</li>
      <li>
	Visualization, with colors representing species / clusters.<br/>
	<img width="600px" src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/660px-Iris_Flowers_Clustering_kMeans.svg.png"/>
      </li>
      <li>In this example, clustering is not that good.</li>
    </ul>
  </li>
  <li>Classification example: Suppose we take a new set of measurements, e.g. (5, 4, 5, 0.1).  What species is this?
    <ul>
      <li>The classification model, given the existing dataset, should be able to answer this question.</li>
      <li>It looks like it could be a setosa.</li>
      <li>A classification algorithm like k-nearest neighbors can make this determination.</li>
    </ul>
  </li>
</ul>

<h2>Computer Vision</h2>
<ul>
  <li>Object detection / recognition is a classification problem.
    <ul>
      <li>Features need to be extracted first.</li>
      <li>These features are fed into a trained classifier, which then tells us what kind of object it is.</li>
      <li>For example, for face detection, we may want features which are characteristic of faces, and when we give it to a classifier of faces, it will tell us whether we have a face or not.</li>
    </ul>
  </li>
  <li>Pixel values are bad feature values.
    <ul>
      <li>If there is a slight perturbation in the image - shifts, rotations, scaling issues - then the pixel values change significantly.</li>
      <li>This makes classification based on pixel intensities very difficult.</li>
    </ul>
  </li>
  <li>Deep learning methods construct hierarchies of features.
    <ul>
      <li>From pixels, we can determine edges, which are better features.</li>
      <li>From edges, we can distinguish shapes, which are even better descriptors.</li>
      <li>A model which forms features hierarchically from other features is a "deep learning model".<br/>
	<img src="http://www.kdnuggets.com/wp-content/uploads/deep-learning.png"/>
      </li>
      <li>Compared to non-deep learning machine learning methods, this more closely models how humans think and perceive.</li>
      <li>This kind of model can be used on other problems, e.g. voice recognition (feature hierarchies can represent utterances, words, phrases, etc.)</li>
    </ul>
  </li>
</ul>
